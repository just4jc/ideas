Objectives
  JD (User Prefs) -> Ranked CVs
  
  User Prefs to express subjectivites (personalised search - learn from history)
  
[Systems]:

JD (User Prefs) -> [Parser] -> Keywords -> [Matcher] -> Selected CV --> [Ranker] --> Ranked CVs

Parser: understands the JD as much as possible and output keywords
  e.g. [job: IT, seniority: senior, role: development]
  
Matcher: Based the the Parser keywords and pick the matched CVs
Expert rules
ML rules

Ranker: sort the CVs according to relevance
Expert ranks
ML ranks: 

Each of the component can have Rule Based (Expert) and/or ML (Machine Learn) part

TF-IDF vs Word2Vec (Frequency vs Co-occurence)

Rule based: 
  pros: direct, specific, accurate, capture domain knowledges
  cons: not scalable, need manual input, 
  
  
Machine Learn:
  pros: automatic, scalable, discover hidden/new relationship
  cons: need a lot of labelled data, hard to achieve high accurancy and low variance
  

Watch-Dog
If a new CV come in that's within the existing top [x], email the notification

Personalized Search
Like buillding recommendation system (collaborative filtering)

Expert Rules
============
- Meaningful phrase: Java Developer, Marketing Manager (can build a bi-gram collocation extraction tool)
- Synonymous: Marketing Manager is also Culture manager 
- Acronym: CFO=Chief Finanical Officer, biz dev=business development
- Preference: Goldman prepare Morgan Stanley, Credit Suisse
* Machine Learning can also help to discover the above rules

Matcher 
====================
 - auto spell correction

- search queries to Elasticsearch for CVs
  e.g: {work-exp contains ("Marketing Manager" or "Culture manager")
        and company contains ("Credit Suisse" or "Morgan Stanely)}

- Contain certain level of fuzziness to expand the search


Labelling
==========
- each labelled data should be prepared by at least 2 person for cross checking and reduce variance
- For each JD/CVs: relevance score (label should be as fine details as possible)
  --> For example: JD desciption is better than general job function (as we can group JD back to job function but not the other way round)
    Relevance score is better than Y/N
  

Test and Performance metrics
============================


Data (JDs, CVs) Pre-processing
==============================
- to small caps
- remove symbols, punctations, stop words
- normalize: Acronym and Synonymous replacement
- identify phases (bigram, trigram...): java_developer, hr_manager


Ranker Algo: job2vec
======================
1. job2vec (j2v): Train by a set of {JD + top[x] words of CV} 
  - top[x] words means the recent experience are most relevant
  - weighted by the labelled score
  - study the word distance and window size in the calculation
  
2. Foreach "Terms" in JD
  -> Get the top[x] related words from j2v with "1/distance" as the Word Score
  
  Example: Term = Web-Developer, Data-Science
  output Word_Socres: 
    Web-Developer = {html5:7.5, javascript:6.4, angular:5.3, php:3.1, .....}
    Data-Science = {python:8.2, machine-learning:7.5, hadoop:6.3,.......}
    
3. Foreach CV of top[x] words (top[x] same as j2v training)
  -> foreach matched_cv_word_cap_by_count[y] in Word_Socres
      CV_socre += Word_Socres[matched_word]
      
   - y: integer of 0..10: cap the score counting of the same word. 0 means no cap
      
4. Ranked by the CV scores
  
- Can we combine with other pre-trained word2vec models (such as skill2vec) with job2vec
- average word embeddings of a phrases now can represent its semantic meaning 
- use document of paragraph vector instead of word?
