Data and Apps
-------------
- In Phrasal translate - add the good data source for training (the movie subtitlies) 
- Wearables like smart watch (measure Pulse, Blood pressure): https://www.mpo-mag.com/issues/2017-06-01/view_columns/the-future-of-medical-wearables/
- use whatsapp msg to train chatbot
- Moblie payment transation used for in credit risk modelling

AWS Slides
---------
11 Amazon 95 ML
34 GPU image

Demos
-------
Use QR code: https://www.the-qrcode-generator.com/
http://ml-playground.com/
http://arogozhnikov.github.io/2016/04/28/demonstrations-for-ml-courses.html
https://js.tensorflow.org/

HK Related
-----------
http://www.scmp.com/business/banking-finance/article/2150150/standard-chartered-says-it-will-apply-operate-virtual-bank
Open API Framework for the Banking Sector and the Launch of Open API on HKMA’s Website
Hong Kong stock exchange enlists AI in fight against rule breakers

NLP
---
Chinese segmentation vs English Prases-identification (collocation)


w2v Word Embedding (Context depends on Corpus)
------------------------
- Context: Mary is a very stubborn child. Her pervicacious nature always gets her in trouble: pervicacious is surrounded by stubborn, nature, and trouble
- learns the meaning of a given word by looking at its context and representing it numerically. By context, we refer to a fixed number of words in front of and behind the word of interest
- ‘a word is characterized by the company it keeps’.
- act as a dimensionality reduction technique (compared to one-hot encoding) but they also give a richer feature representation
- Traditional linguistic like learning grammar from school
- Word Emedding like native speaker (doesn't learn grammar) just pick up on what other people talk
- word embedding vs keyword correlation (co-occurrence)
	multi-dimension vs 1-dimenson: hence can calculate angle and more interesting arithmetic
	w2v have the window lenght to determine the "relevant" context

- Steps: https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-word2vec-e0128a460f0f

- Demo: Gensim with tensorboard visualize

- structured skip-gram: capture the "strength" of association by considering word positions

- https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/ : ITEM_3469' + 'pregnant' example

- http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html
  Pointwise mutual information (PMI): how much more likely we get a pair than if it were at random
  (vectorize words to a certain dimension is a compression) compression is lossy may give it an advantage, as it can discover patterns rather than only memorize each pair. For example, in recommendation systems for movie ratings, each rating is approximated by a scalar product of two vectors - a movie’s content and a user’s preference. This is used to predict scores for as yet unseen movies
   word2vec is not a single task or algorithm; popular ones are:
	Skip-Gram Negative-Sampling (implicit compression of PMI: factorizes a word-context PMI matrix),
	Skip-Gram Noise-Contrastive Training (implicit compression of conditional probability),
	GloVe (explicit compression of co-occurrences)

- https://nlpers.blogspot.com/2016/06/language-bias-and-black-sheep.html
  co-occurance frequencies of words definitely do not reflect co-occurance frequencies of things in the real world (e.g. trying to model a "default" as something that doesn't appear won't work)

- A vector is worth thousands of words

TF-IDF
-----
- often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. 
- The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus (document importance grows with the local frequency and global rarity of its terms)

 - Example: Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.

Use case: https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41

Sentiment Case: Study Smart Beta 
--------------------------------
https://socialmarketanalytics.wordpress.com/2015/10/27/social-media-and-smart-beta/
•	SPDR SPY ETF is a cap-weighted ETF which closely replicates the performance of the S&P 500.
•	Develop a “smart beta” strategy using the social media sentiment levels of individuals ETF constituents to rebalance the weights of the constituents in the ETF while keeping the Assets under Management constant
•	The transaction cost assumption is ignored for both the original and the enhanced ETF
•	Strategy 1: Looking at the sentiment levels an hour before the close (2:55 PM Eastern Time) and re-balancing the weights according to that. The stocks were bought or sold (to reduce position as per new weight only, NO short selling) at the close of the day and the positions were maintained until the next day when the re-balancing was performed again.
•	Strategy 2:  use a “lagged” sentiment (the lag being a day). So, for adjusting the weights today, we looked at the sentiment at 2:55 PM yesterday, and changed the positions based on that.


JPM - Extracting Sentiment.pdf
-----------------------------
word embeddings
p.14, 16: most similar & disimilar
p.17,18: word vector addition and substraction
p.19, 55(apple): comparing corpus (i.e. context)
p.23: feature pipeline

Sentiment
----------
- Stanford example: use sentence tree to solve word negation ("not good" -> negative)


Matlab 
-------
Example to separate training and test data 
Text Analytics Toolbox

Images
-------
Plutchik's wheel of emotion: http://evome.co/wp-content/uploads/2017/05/thewheel.jpg
Random Forest: https://cdn-images-1.medium.com/max/800/1*Sp2xGfv0uw3Nq5Qw7gJ6mw.png
Feature Extraction: https://cdn-images-1.medium.com/max/1000/1*7n6g6yeEIOrXL6HnybmZdA.jpeg
Deduction vs Induction: https://cdn-images-1.medium.com/max/800/1*K9wPLf21MuJ04bhO-To_yg.png


Performance, Metrics and Misleading Examples
-----------------------------------------------
Using accuracy as measurements is not a good idea with unbalanced data

https://www.independent.co.uk/news/world/americas/us-elections/mogia-ai-system-that-correctly-predicted-last-3-us-elections-says-donald-trump-will-win-artificial-a7384671.html?amp

https://towardsdatascience.com/how-to-lie-with-data-science-5090f3891d9c
	Feature engineering/selection leaks
	Dependent data leak
	Chance/Luck
	

Trading
-------
- Trader said, retail driven market 5, 10 price will be thick queue, place a tick above -> data of hidden psychology
- Shorter the duration - easier to predict. Combined signals, combinatorial, grid search
- Many mini Black swan (long tail event) rip off the small profit(s)

ITG
---
limit order posting algorithm contains a short-term alpha prediction model based on random forest classification
modeling of historic volume patterns leverages a k-means clustering technique

JPM Active Learning
-------------------------------
- avoids hand crafting of logic / manual overfitting
- allows for strategies to be optimised around characteristics of individual stocks
- but this is not just about data and compute : domain expertise critical for applications in finance

Why Backtest not good
--------------------
1.	Each backtest is always overfit to some extent
2.	Transactions cost, Spread 
3.	No Liquidity at that price
4.	Volatility and latency - can’t react fast enough (not have the best in class infra) 
5.	(mini) Black swan (win candy, lose factory)

==> Paper Trading

Amateurs develop individual strategies, believing that there is such a thing as a magical formula for riches, Combine multiple predictions into a single bet

confuse statistical flukes with patterns. This fact, combined with the low signal-to-noise ratio that characterizes finance

BlackJack (player lost immediately when go bust) vs CBBC (Callable bull/bear contract)

alpha– streams of uncorrelated risk-adjusted returns
exploit these returns via a systematic trading model and execution infrastructure.

Alpha is difficult to find, as by definition once it is well-known it decays and seeks to be
an uncorrelated source of returns (like Orgasm, it goes away if thinking it too much)

Signal building blocks : for example, the bid/ask imbalance for short term price movement, not going to help you to make money (bid/ask spread) But can be used in the fine-tuning the strategy entry and exit timing

How much you should help the model to learn

- Predicting stock going up or down is too idealistic, generate "better" signals (build blocks)

- Instead of finding quick answer, rather it can disapprove it quickly

- Instead of know which factor , rather it can find the right combinations (magic number) of those factor


Advance Algorithm Trading book
------------------------------
Bayesian Statistics takes a different approach and instead considers probability as a measure of belief

The main idea in Time Series Analysis is that of serial correlation (momentum, trending following, “mean” reversion, seasonality, volatility clustering, long-memory effects)

Machine Learning - fusion of computational methods–mainly optimisation techniques–within a rigourous probabilistic framework. It provides the ability to "learn a model from data".

Bayesian Approach stochastic volatility - regime detection

Cointegration – pairs trading

https://www.quantstart.com/qstrader

Stochastic vs not enough feature

Historical data how far going back vs regime switching

Reading many roughly vs a few precisely (Phone Survey vs Social Media Analytics)

Observer Effect: the observations made for the measurement of the state of the system change the very state they intend to measure. Financial markets, where the primary observations come into the picture by way of order submissions, represent a poster-child for the Observer Effect – increasingly so due to the immense computational power and growing sophistication behind trading operations. Thus, for observations to render value in environments such as this, one needs to act simultaneously while observing them.

Companies
-------------
https://www.interactivebrokers.com
https://www.multicharts.com

Blogs
-----
https://www.elitetrader.com/et/
http://www.trade2win.com/
http://www.traderslaboratory.com/forums/

Numer.ai
--------
https://numer.ai/
https://medium.com/numerai/numerais-master-plan-1a00f133dba9
https://numer.ai/learn
https://medium.com/numerai/encrypted-data-for-efficient-markets-fffbe9743ba8

Using just Yahoo! Finance data to build a model is like using only one pixel in an image to learn to recognize handwritten digits

Once you have a model in finance that works, you hide it. You hide the techniques you used to build it. You hide the methods you used to improve your data. And most importantly, you hide the data. The financial incentive for secrecy is strong.

Numerai is now trading user generated predictions in our hedge fund

https://medium.com/numerai/invisible-super-intelligence-for-the-stock-market-3c64b57b244c

(AdaBoost Example?) With many different solutions to the same problem, Numerai is able to combine each model into a meta model just like Random Forests combines decision trees into a forest.

No data scientist on Numerai has a machine learning model that is better than all the other models combined. So Numerai is not a search for the ‘best’ model; it is a platform to synthesize many different models with many different characteristics. Although data scientists compete to place on the leaderboard, the competition is designed to collect models. Numerai is not really a competition; it’s an invisible collaboration to build the meta model.

https://medium.com/@pycy/million-dollar-salaries-for-ai-researchers-well-we-quants-have-seen-this-movie-before-8e7af51f6c63

Different funds take varying approaches to the arms race. While funds such as the ultra-successful and ultra-secretive Renaissance Technologies continue to outperform by hiring relatively few (<100) elite researchers, others such as WorldQuant have taken the opposite approach, hiring teams of hundreds of mutually segregated quants working independently, allowing the firm to collect to the order of 100,000 alpha signals. Still other funds such as Quantopian take this to the extreme — and have attempted to completely crowd-source quantitative research (with questionable success). Vs Social Trading


AI study
--------
Baseline comparison:  Bid/Ask imbalance vs 50/50 is not correct (vs Bid>Ask -> Up; Bid<Ask-> Down)

https://www.cnbc.com/2018/04/09/chinese-ai-startup-sensetime-raises-600-million-from-alibaba-others.html
https://chiefscientist.org/francois-chollet-in-conversation-with-alexy-khrabrov-84f0358ec651

Not generalize well  (一理通,百理明)
only local generalization generalize to things that are very close to what you’ve been trained on and that’s extremely limiting. That’s the reason why deep learning is only going to work well on very large datasets.

We need a dense sampling of input/output space to learn a specific task. A very narrow task

This abstract model comes from grounding, comes from the human experience. And the deep learning model has no access to human experiences. It just has access to its training data. So, again, all they’re doing is mapping the statistics of their training data. And it’s not what humans do. First, because humans have access to these very different set of experiences, and, second, because they learn in very different ways. Humans can automatically turn what they perceive into abstract models. They can form abstract models automatically, which is something you cannot do with machine learning today.

But because it has no access to abstract rules, it’s not going to be able to generalize strongly.

- Imbalance dataset, Precision vs Recall

Feature engineering is more difficult because it’s domain-specific, while learners (ML models) can be largely general-purpose

Bias is a learner’s tendency to consistently learn the same wrong thing. Variance is the tendency to learn random things irrespective of the real signal
A linear learner has high bias, because when the frontier between two classes is not a hyperplane the learner is unable to induce it. Decision trees don’t have this problem because they can represent any Boolean function, but on the other hand they can suffer from high variance: decision trees learned on different training sets generated by the same phenomenon are often very different, when in fact they should be the same.

statistics is the science of changing your mind
Type I error is changing your mind when you shouldn’t.
Type II error is NOT changing your mind when you should.
Type I error is like convicting an innocent person and Type II error is like failing to convict a guilty person.
What’s a Type III error? It’s kind of a statistics joke: it refers to correctly rejecting the wrong null hypothesis. In other words, using all the right math to answer the wrong question.

GAN: reconstruct blurry or pixelated images without seeing the original
https://news.developer.nvidia.com/ai-can-now-reconstruct-blurry-images/

Reinforcement Learning: Add JPM Active trading slides 
The Advantage of Doubling: A Deep Reinforcement Learning Approach to Studying the Double Team in the NBA: https://arxiv.org/abs/1803.02940

The success of ML project is where the you have has a lot of (good/labelled) data (like Facebook, Google) or you have some unique data that generate from the biz. ML algos are standardized, it's the data that embedd the knowledge and choose suitable model to extract it

Some Big Data techniquies are solving internet scale data volume and growth and doesn't mean you will use it directly
Deep Learning are able to understand massive amout of data and that doesn't mean you will use it directly

AI replace jobs, works 3-day a week: Social Issues -> Socially solve;  tech is tech
Just because we have open government data doesn’t mean we don’t have corruption

Training time (vs predict time) is generally longer and then cross validation is more time consumption when you have big data set to train

Is it a race between rabbit & tortoise, but tortoise cannot fly (limitation of the model)

Andrew Ng has gone on the record stating that worrying about superintelligence is like worrying about overpopulation on Mars. One day, humanity is likely to reach Mars. When enough people land on Mars, overcrowding will likely exist and may even be a very serious problem. None of this changes the fact that Mars today is an empty wasteland.

- GPU: training batch size: speed (parellelism) vs memory


Deep Learning Challenges
---------------------------
- Data hungry (need lot of quality data set)
- Not generalize well (need to see many variation)
--> Compare this to human learning to cross the road


AI Thoughts
---------
- Discover from data and only data can provide it
- you think there is some relation and you want to find "how" (or at least to disapprove your thinking)

- Deep Learning need to learn with fewer data

- Machine Learning with experience in memory (no need to train from the beginning every time), aka accumulate knowledge

- Similar to Algo trading 
	Not as good as people but cut cost
		
	automate some "dealer" ("analysts")
			
	something computer is genuinely better (speed and read many data)

Social Media
------------
social media companies get ever greater visibility into our lives and minds. At the same time, they gain increasing access to behavioral control vectors — in particular via algorithmic newsfeeds, which control our information consumption

Cat Son's coomments on FB newsfeed, it's like praise your mum for one dish and she cooks the same dishes almost everyday

Predicting Love and Breakups with Facebook Data: https://techcrunch.com/2014/02/14/facebook-love-data/

slide 2: Social Keyword Index 
Datatact word correlation (like word embedding) to choose top words for certain things (companies, events etc)
Calculate the sentiment of those keywords as index


Information Theory
-----------------
Shannon proposed that the “semantic aspects of data are irrelevant”, and nature and meaning of data doesn’t matter when it comes to information content. Instead he quantified information in terms of probability distribution and “uncertainty”

more inherent uncertainty in the experiment then it has higher entropy


Time Series
-----------
- “stock index”, was actually modeled using a random walk process (a completely stochastic process), the idea of using historical data as a training set in order to learn the behavior and predict future outcomes is simply not possible

- autocorrelations: the index at time “t+1” is quite likely close to the index at time “t”

- being able to predict the time-differenced data, rather than the data directly, is a much stronger indication of the predictive power of the model


Building the Software 2.0 Stack
--------------------------------
52. Amout of lost sleep over
54-75. challenges in labelling dataset


Stats
------
likelihood (asking about the parameter values) vs probability (asking about the data) 
	- the likelihood of the parameters μ and σ taking certain values given that we’ve observed a bunch of data
	- the probability density of observing the data with model parameters μ and σ
	
Bayesian: allows us to use some knowledge or belief that we already have (commonly known as the prior) to help us calculate the probability of a related event


Training Dataset
---------------
variation
relation


Misc
-----
AlphaGo Video timing and (show and ask) why it take a lot of time to think at the beginning
sunspring youtube

Quantamental Investing

Classifiers are just complicated if-statements

Machine learning has moved past the high-school sex phase (everyone talking about it, but no one doing it), to a much broader implementations across industries including media.
 
 “All models are wrong, but some are useful.” — George Box
  More Data Beats a Cleverer Algorithm
  All learners essentially work by grouping nearby examples into the same class; the key difference is in the meaning of “nearby.”

analyst reports our word embeddings are more relevant to equities (the original Google model is more general purpose where for example ‘overweight’ is mapped closer to ‘obese’ but in our model it is closer to the word ‘bullish’)

Machine Learning workflow = Edge of Tomorrow Movie

Bringing powerful data-mining to bear on noisy data tends to result in overfit. That is, the models fit more to the noise than to the signal. This results in inaccurate trading signals.

News
----
https://economictimes.indiatimes.com/markets/stocks/news/jpmorgan-brings-amazons-alexa-to-wall-street-trading-floors/articleshow/63541706.cms?from=mdr

https://www.investopedia.com/news/jim-rogers-launches-aidriven-etf/

http://www.marketing-interactive.com/bloomberg-and-twitter-to-incorporate-financially-relevant-content-into-trading/?utm_campaign=20180719_mktdaily&utm_medium=email&utm_source=HK&utm_content=listing


State of AI - 201806
--------------------
13: The Information Theory: First memorise the data, then forget what doesn’t help the model make predictions
40-43: Fairness in machine learning: How do we ensure our models are not biased?
53: AI to automate away AI engineers. Google’s AutoML automatically discovers the best model architecture
72: Vendor AI cloud services
85: Privacy preservation and data anonymisation

China Internet Report
---------------------
5: Top China (incl HK) startup cities
6-10: Chinese companies
13-14: BAT
17-18: Social Apps (Social Group Buy, incentive based sharing) *** Add to slide-1
39-40: WeChat mini-program so that can leverage WeChat Pay and the ecosystem
45: Bike-sharing, used to be more than 40
52: Facial Reg examples
53: Farming (running chicken == tamagotchi)
76: fintech

