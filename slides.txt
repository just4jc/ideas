GAN: reconstruct blurry or pixelated images without seeing the original
https://news.developer.nvidia.com/ai-can-now-reconstruct-blurry-images/

Facial Recognition example: Thanks To AI, A 3rd Person Is Arrested Following A Pop Superstar's Concert https://www.npr.org/sections/therecord/2018/05/23/613692526/thanks-to-ai-a-3rd-person-is-arrested-following-a-pop-superstars-concert

Quantamental Investing

Reinforcement Learning: Add JPM Active trading slides
The Advantage of Doubling: A Deep Reinforcement Learning Approach to Studying the Double Team in the NBA: https://arxiv.org/abs/1803.02940

In Phrasal translate - add the good data source for training (the movie subtitlies) 

Add to History of AI: http://houseofbots.com/news-detail/2754-1-a-take-on-deep-learning

The success of ML project is where the you have has a lot of (good/labelled) data (like Facebook, Google) or you have some unique data that generate from the biz. ML algos are standardized, it's the data that embedd the knowledge and choose suitable model to extract it

Some Big Data techniquies are solving internet scale data volume and growth and doesn't mean you will use it directly
Deep Learning are able to understand massive amout of data and that doesn't mean you will use it directly

AI replace jobs, works 3-day a week: Social Issues -> Socially solve;  tech is tech
Just because we have open government data doesn’t mean we don’t have corruption

Training time (vs predict time) is generally longer and then cross validation is more time consumption when you have big data set to train

Human Data Labeller: https://theinitium.com/article/20180522-mainland-data-annotator/

Add "Buy Likes" Phone images to Datatact FB a/c and the Fake Sentiment

HK companies from class Web site

the keyword correlation (co-occurrence) is like word embedding

Misleading example:
https://www.independent.co.uk/news/world/americas/us-elections/mogia-ai-system-that-correctly-predicted-last-3-us-elections-says-donald-trump-will-win-artificial-a7384671.html?amp

slide 1: Include key datatact stats, COL biz plan, 

slide 2: Socal Keyword Index 

Ensemble / Social Trading / Numer.ai: wisdom of the crowd, 三個臭皮匠，勝過一個諸葛亮 (3 idiots are better than one smart guy)?
Is it a race between rabbit & tortoise, but tortoise cannot fly (limitation of the model)


Trading:
- Trader said, retail driven market 5, 10 price will be thick queue, place a tick above -> data of hidden psychology
- Shorter the duration - easier to predict. Combined signals, combinatorial, grid search
- Many mini Black swan (long tail event) rip off the small profit(s)

Word2Vec show in Text Similarity: http://projector.tensorflow.org/
Demo the Gensim example

TF-IDF ranking is that a document importance grows with the local frequency and global rarity of its terms

Tf-idf: term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. 

  Example: Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.


ITG
---
limit order posting algorithm contains a short-term alpha prediction model based on random forest classification
modeling of historic volume patterns leverages a k-means clustering technique

JPM Active Learning
-------------------------------
- avoids hand crafting of logic / manual overfitting
- allows for strategies to be optimised around characteristics of individual stocks
- but this is not just about data and compute : domain expertise critical for applications in finance

Why Backtest not good
--------------------
1.	Each backtest is always overfit to some extent
2.	Transactions cost, Spread 
3.	No Liquidity at that price
4.	Volatility and latency - can’t react fast enough (not have the best in class infra) 
5.	(mini) Black swan (win candy, lose factory)

==> Paper Trading

Amateurs develop individual strategies, believing that there is such a thing as a magical formula for riches, Combine multiple predictions into a single bet

confuse statistical flukes with patterns. This fact, combined with the low signal-to-noise ratio that characterizes finance

BlackJack (player lost immediately when go bust) vs CBBC (Callable bull/bear contract)

alpha– streams of uncorrelated risk-adjusted returns
exploit these returns via a systematic trading model and execution infrastructure.

Alpha is difficult to find, as by definition once it is well-known it decays and seeks to be
an uncorrelated source of returns (like Orgasm, it goes away if thinking it too much)

Signal building blocks : for example, the bid/ask imbalance for short term price movement, not going to help you to make money (bid/ask spread) But can be used in the fine-tuning the strategy entry and exit timing

How much you should help the model to learn

- Predicting stock going up or down is too idealistic, generate "better" signals (build blocks)

- Instead of finding quick answer, rather it can disapprove it quickly

- Instead of know which factor , rather it can find the right combinations (magic number) of those factor


Advance Algorithm Trading book
------------------------------
Bayesian Statistics takes a different approach and instead considers probability as a measure of belief

The main idea in Time Series Analysis is that of serial correlation (momentum, trending following, “mean” reversion, seasonality, volatility clustering, long-memory effects)

Machine Learning - fusion of computational methods–mainly optimisation techniques–within a rigourous probabilistic framework. It provides the ability to "learn a model from data".

Bayesian Approach stochastic volatility - regime detection

Cointegration – pairs trading

https://www.quantstart.com/qstrader

Andrew Ng has gone on the record stating that worrying about superintelligence is like worrying about overpopulation on Mars. One day, humanity is likely to reach Mars. When enough people land on Mars, overcrowding will likely exist and may even be a very serious problem. None of this changes the fact that Mars today is an empty wasteland.


Sentiment Case: Study Smart Beta 
--------------------------------
https://socialmarketanalytics.wordpress.com/2015/10/27/social-media-and-smart-beta/
•	SPDR SPY ETF is a cap-weighted ETF which closely replicates the performance of the S&P 500.
•	Develop a “smart beta” strategy using the social media sentiment levels of individuals ETF constituents to rebalance the weights of the constituents in the ETF while keeping the Assets under Management constant
•	The transaction cost assumption is ignored for both the original and the enhanced ETF
•	Strategy 1: Looking at the sentiment levels an hour before the close (2:55 PM Eastern Time) and re-balancing the weights according to that. The stocks were bought or sold (to reduce position as per new weight only, NO short selling) at the close of the day and the positions were maintained until the next day when the re-balancing was performed again.
•	Strategy 2:  use a “lagged” sentiment (the lag being a day). So, for adjusting the weights today, we looked at the sentiment at 2:55 PM yesterday, and changed the positions based on that.

Stochastic vs not enough feature

Historical data how far going back vs regime switching

Reading many roughly vs a few precisely (Phone Survey vs Social Media Analytics)

Observer Effect: the observations made for the measurement of the state of the system change the very state they intend to measure. Financial markets, where the primary observations come into the picture by way of order submissions, represent a poster-child for the Observer Effect – increasingly so due to the immense computational power and growing sophistication behind trading operations. Thus, for observations to render value in environments such as this, one needs to act simultaneously while observing them.

Companies
-------------
https://www.interactivebrokers.com
https://www.multicharts.com

Blogs
-----
https://www.elitetrader.com/et/
http://www.trade2win.com/
http://www.traderslaboratory.com/forums/

Numer.ai
--------
https://numer.ai/
https://medium.com/numerai/numerais-master-plan-1a00f133dba9
https://numer.ai/learn
https://medium.com/numerai/encrypted-data-for-efficient-markets-fffbe9743ba8

Using just Yahoo! Finance data to build a model is like using only one pixel in an image to learn to recognize handwritten digits

Once you have a model in finance that works, you hide it. You hide the techniques you used to build it. You hide the methods you used to improve your data. And most importantly, you hide the data. The financial incentive for secrecy is strong.

Numerai is now trading user generated predictions in our hedge fund

https://medium.com/numerai/invisible-super-intelligence-for-the-stock-market-3c64b57b244c

(AdaBoost Example?) With many different solutions to the same problem, Numerai is able to combine each model into a meta model just like Random Forests combines decision trees into a forest.

No data scientist on Numerai has a machine learning model that is better than all the other models combined. So Numerai is not a search for the ‘best’ model; it is a platform to synthesize many different models with many different characteristics. Although data scientists compete to place on the leaderboard, the competition is designed to collect models. Numerai is not really a competition; it’s an invisible collaboration to build the meta model.

https://medium.com/@pycy/million-dollar-salaries-for-ai-researchers-well-we-quants-have-seen-this-movie-before-8e7af51f6c63

Different funds take varying approaches to the arms race. While funds such as the ultra-successful and ultra-secretive Renaissance Technologies continue to outperform by hiring relatively few (<100) elite researchers, others such as WorldQuant have taken the opposite approach, hiring teams of hundreds of mutually segregated quants working independently, allowing the firm to collect to the order of 100,000 alpha signals. Still other funds such as Quantopian take this to the extreme — and have attempted to completely crowd-source quantitative research (with questionable success). Vs Social Trading


AI study
--------
Baseline comparison:  Bid/Ask imbalance vs 50/50 is not correct (vs Bid>Ask -> Up; Bid<Ask-> Down)

https://www.cnbc.com/2018/04/09/chinese-ai-startup-sensetime-raises-600-million-from-alibaba-others.html
https://chiefscientist.org/francois-chollet-in-conversation-with-alexy-khrabrov-84f0358ec651

Not generalize well  (一理通,百理明)
only local generalization generalize to things that are very close to what you’ve been trained on and that’s extremely limiting. That’s the reason why deep learning is only going to work well on very large datasets.

We need a dense sampling of input/output space to learn a specific task. A very narrow task

This abstract model comes from grounding, comes from the human experience. And the deep learning model has no access to human experiences. It just has access to its training data. So, again, all they’re doing is mapping the statistics of their training data. And it’s not what humans do. First, because humans have access to these very different set of experiences, and, second, because they learn in very different ways. Humans can automatically turn what they perceive into abstract models. They can form abstract models automatically, which is something you cannot do with machine learning today.

But because it has no access to abstract rules, it’s not going to be able to generalize strongly.

AlphaGo Video timing and (show and ask) why it take a lot of time to think at the beginning
sunspring youtube



