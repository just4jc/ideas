Learning to Ranking Candidate.pdf
==================================
- Workflow
The automatic CV parsing extracts relevant information such as
name, address, skills and previous work experience from original
CVs (e. g. given as PDF or DOC documents) and transforms
them into a searchable semi-structured format.

The search engine indexes parsed CVs and enables searching
with semi-structured queries as well as through search facets
and tag clouds. CVs are assigned a relevance score w.r.t. the
query by the search engine and are ranked accordingly.

Automatic vacancy parsing extracts relevant information from
vacancies such as the title of the advertised position, skill requirements
and other job-opening-related keywords.

The query generation component automatically generates semistructured
search queries for finding matching candidates in the
document collection.

- Relevant
match-making system:
  Also from candidate point of view: whether job suitable as their next career step
  it is not only the preferences of the recruiter that matters but also the preferences of the candidate
  Take into account the probability of the candidate accepting the job offered to them as well as the probability of the candidate remaining with the organisation for long term, for both of which they have explicit historical data to learn from.
  

Word2Vec
========
Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word

Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model).

CBOW (continuous bag-of-words): architecture, the model predicts the current word from a window of surrounding context words

Skip-gram: predict surrounding context words based on the current word.


skill2Vec
=========
input layer: one-hot encoding with size of the dictionary
output layer: softmax classifier, predict words within window size before and after current word
params: vector dimensionality and window size

The author found that increasing the window size improves the quality of the word vector, and yet it increases the computational complexity

Dataset: 1. standard skills dictionary for the parser 2. skills for training model

treated our skills as words in Word2Vec model.
In this study, with the documents contain only the skills,
we chose the maximum window size, implied that every skills in the same job description are related to each other

AI to idenitying skill relationship
=====================================
- relationship between the knowledge or skills based on the learning data available
- transform an input skill to its contextual skills. In our approach, contextual skills refer to the neighbours of a skill in the sequence
- find a mapping from a skill to a vector in a new vector space that preserves the structure between different skills
- This mapping, called an “embedding,” allows the identification of the relationship between skills by projecting the skills onto the new vector space. The results of an embedding can also be visualized by further projecting the vectors in the new space onto a two-dimensional space.
- The rationale of skill2vec is that for each skill St in the sequence, the contextual skills are defined as skills for solving the neighboring tasks within a window of size W (skip-gram approach)

Mine related tech skills
========================
We also tried using the resume dataset, but the results were of a lower quality, as the skills extracted from resumes can be from different jobs

Relevant Serach: With Application of Solr and Elasticsearch
============================================================
latent semantic analysis
latent Dirichlet allocation
Word2vec

